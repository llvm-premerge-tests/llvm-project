; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 2
; RUN: opt < %s -passes=infer-alignment -S | FileCheck %s

; Check that assume is propagated backwards through all
; operations that are `isGuaranteedToTransferExecutionToSuccessor`
; (it should reach the load and mark it as `align 32`).
define i32 @assume_inevitable(ptr %a, ptr %b, ptr %c) {
; CHECK-LABEL: define i32 @assume_inevitable
; CHECK-SAME: (ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]]) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[DUMMY:%.*]] = alloca i8, align 4
; CHECK-NEXT:    [[M:%.*]] = alloca i64, align 8
; CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 32
; CHECK-NEXT:    [[LOADRES:%.*]] = load i32, ptr [[B]], align 4
; CHECK-NEXT:    store i32 [[LOADRES]], ptr [[A]], align 32
; CHECK-NEXT:    [[OBJSZ:%.*]] = call i64 @llvm.objectsize.i64.p0(ptr [[C]], i1 false, i1 false, i1 false)
; CHECK-NEXT:    store i64 [[OBJSZ]], ptr [[M]], align 8
; CHECK-NEXT:    [[PTRINT:%.*]] = ptrtoint ptr [[A]] to i64
; CHECK-NEXT:    [[MASKEDPTR:%.*]] = and i64 [[PTRINT]], 31
; CHECK-NEXT:    [[MASKCOND:%.*]] = icmp eq i64 [[MASKEDPTR]], 0
; CHECK-NEXT:    tail call void @llvm.assume(i1 [[MASKCOND]])
; CHECK-NEXT:    ret i32 [[TMP0]]
;
entry:
  %dummy = alloca i8, align 4
  %m = alloca i64
  %0 = load i32, ptr %a, align 4

  %loadres = load i32, ptr %b
  store i32 %loadres, ptr %a

  %objsz = call i64 @llvm.objectsize.i64.p0(ptr %c, i1 false)
  store i64 %objsz, ptr %m

  %ptrint = ptrtoint ptr %a to i64
  %maskedptr = and i64 %ptrint, 31
  %maskcond = icmp eq i64 %maskedptr, 0
  tail call void @llvm.assume(i1 %maskcond)

  ret i32 %0
}

declare i64 @llvm.objectsize.i64.p0(ptr, i1)
declare void @llvm.assume(i1)
