// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 2
// RUN: %clang_cc1 -triple riscv64-none-linux-gnu -target-feature +zve64d \
// RUN: -target-feature +f -target-feature +d -target-feature +zvfh -target-feature +zfh -disable-O0-optnone \
// RUN: -mvscale-min=4 -mvscale-max=4 -emit-llvm -o - %s | \
// RUN: opt -S -passes=sroa | FileCheck %s
// REQUIRES: riscv-registered-target

#include <riscv_vector.h>

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_abs
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_ABS:%.*]] = call <vscale x 8 x i8> @llvm.abs.nxv8i8(<vscale x 8 x i8> [[VI8]], i1 false)
// CHECK-NEXT:    [[ELT_ABS1:%.*]] = call <vscale x 4 x i16> @llvm.abs.nxv4i16(<vscale x 4 x i16> [[VI16]], i1 false)
// CHECK-NEXT:    [[ELT_ABS2:%.*]] = call <vscale x 2 x i32> @llvm.abs.nxv2i32(<vscale x 2 x i32> [[VI32]], i1 false)
// CHECK-NEXT:    [[ELT_ABS3:%.*]] = call <vscale x 1 x i64> @llvm.abs.nxv1i64(<vscale x 1 x i64> [[VI64]], i1 false)
// CHECK-NEXT:    [[ELT_ABS4:%.*]] = call <vscale x 4 x half> @llvm.fabs.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_ABS5:%.*]] = call <vscale x 2 x float> @llvm.fabs.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_ABS6:%.*]] = call <vscale x 1 x double> @llvm.fabs.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_abs(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vint8m1_t res_vi8 = __builtin_elementwise_abs(vi8);
  vint16m1_t res_vi16 = __builtin_elementwise_abs(vi16);
  vint32m1_t res_vi32 = __builtin_elementwise_abs(vi32);
  vint64m1_t res_vi64 = __builtin_elementwise_abs(vi64);
  vfloat16m1_t res_vf16 = __builtin_elementwise_abs(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_abs(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_abs(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_add_sat
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_SAT:%.*]] = call <vscale x 8 x i8> @llvm.sadd.sat.nxv8i8(<vscale x 8 x i8> [[VI8]], <vscale x 8 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_SAT1:%.*]] = call <vscale x 4 x i16> @llvm.sadd.sat.nxv4i16(<vscale x 4 x i16> [[VI16]], <vscale x 4 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_SAT2:%.*]] = call <vscale x 2 x i32> @llvm.sadd.sat.nxv2i32(<vscale x 2 x i32> [[VI32]], <vscale x 2 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_SAT3:%.*]] = call <vscale x 1 x i64> @llvm.sadd.sat.nxv1i64(<vscale x 1 x i64> [[VI64]], <vscale x 1 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_SAT4:%.*]] = call <vscale x 8 x i8> @llvm.uadd.sat.nxv8i8(<vscale x 8 x i8> [[VU8]], <vscale x 8 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_SAT5:%.*]] = call <vscale x 4 x i16> @llvm.uadd.sat.nxv4i16(<vscale x 4 x i16> [[VU16]], <vscale x 4 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_SAT6:%.*]] = call <vscale x 2 x i32> @llvm.uadd.sat.nxv2i32(<vscale x 2 x i32> [[VU32]], <vscale x 2 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_SAT7:%.*]] = call <vscale x 1 x i64> @llvm.uadd.sat.nxv1i64(<vscale x 1 x i64> [[VU64]], <vscale x 1 x i64> [[VU64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_add_sat(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vint8m1_t res_vi8 = __builtin_elementwise_add_sat(vi8, vi8);
  vint16m1_t res_vi16 = __builtin_elementwise_add_sat(vi16, vi16);
  vint32m1_t res_vi32 = __builtin_elementwise_add_sat(vi32, vi32);
  vint64m1_t res_vi64 = __builtin_elementwise_add_sat(vi64, vi64);
  vuint8m1_t res_vu8 = __builtin_elementwise_add_sat(vu8, vu8);
  vuint16m1_t res_vu16 = __builtin_elementwise_add_sat(vu16, vu16);
  vuint32m1_t res_vu32 = __builtin_elementwise_add_sat(vu32, vu32);
  vuint64m1_t res_vu64 = __builtin_elementwise_add_sat(vu64, vu64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_sub_sat
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_SAT:%.*]] = call <vscale x 8 x i8> @llvm.ssub.sat.nxv8i8(<vscale x 8 x i8> [[VI8]], <vscale x 8 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_SAT1:%.*]] = call <vscale x 4 x i16> @llvm.ssub.sat.nxv4i16(<vscale x 4 x i16> [[VI16]], <vscale x 4 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_SAT2:%.*]] = call <vscale x 2 x i32> @llvm.ssub.sat.nxv2i32(<vscale x 2 x i32> [[VI32]], <vscale x 2 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_SAT3:%.*]] = call <vscale x 1 x i64> @llvm.ssub.sat.nxv1i64(<vscale x 1 x i64> [[VI64]], <vscale x 1 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_SAT4:%.*]] = call <vscale x 8 x i8> @llvm.usub.sat.nxv8i8(<vscale x 8 x i8> [[VU8]], <vscale x 8 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_SAT5:%.*]] = call <vscale x 4 x i16> @llvm.usub.sat.nxv4i16(<vscale x 4 x i16> [[VU16]], <vscale x 4 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_SAT6:%.*]] = call <vscale x 2 x i32> @llvm.usub.sat.nxv2i32(<vscale x 2 x i32> [[VU32]], <vscale x 2 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_SAT7:%.*]] = call <vscale x 1 x i64> @llvm.usub.sat.nxv1i64(<vscale x 1 x i64> [[VU64]], <vscale x 1 x i64> [[VU64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_sub_sat(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vint8m1_t res_vi8 = __builtin_elementwise_sub_sat(vi8, vi8);
  vint16m1_t res_vi16 = __builtin_elementwise_sub_sat(vi16, vi16);
  vint32m1_t res_vi32 = __builtin_elementwise_sub_sat(vi32, vi32);
  vint64m1_t res_vi64 = __builtin_elementwise_sub_sat(vi64, vi64);
  vuint8m1_t res_vu8 = __builtin_elementwise_sub_sat(vu8, vu8);
  vuint16m1_t res_vu16 = __builtin_elementwise_sub_sat(vu16, vu16);
  vuint32m1_t res_vu32 = __builtin_elementwise_sub_sat(vu32, vu32);
  vuint64m1_t res_vu64 = __builtin_elementwise_sub_sat(vu64, vu64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_max
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_MAX:%.*]] = call <vscale x 8 x i8> @llvm.smax.nxv8i8(<vscale x 8 x i8> [[VI8]], <vscale x 8 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_MAX1:%.*]] = call <vscale x 4 x i16> @llvm.smax.nxv4i16(<vscale x 4 x i16> [[VI16]], <vscale x 4 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_MAX2:%.*]] = call <vscale x 2 x i32> @llvm.smax.nxv2i32(<vscale x 2 x i32> [[VI32]], <vscale x 2 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_MAX3:%.*]] = call <vscale x 1 x i64> @llvm.smax.nxv1i64(<vscale x 1 x i64> [[VI64]], <vscale x 1 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_MAX4:%.*]] = call <vscale x 8 x i8> @llvm.umax.nxv8i8(<vscale x 8 x i8> [[VU8]], <vscale x 8 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_MAX5:%.*]] = call <vscale x 4 x i16> @llvm.umax.nxv4i16(<vscale x 4 x i16> [[VU16]], <vscale x 4 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_MAX6:%.*]] = call <vscale x 2 x i32> @llvm.umax.nxv2i32(<vscale x 2 x i32> [[VU32]], <vscale x 2 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_MAX7:%.*]] = call <vscale x 1 x i64> @llvm.umax.nxv1i64(<vscale x 1 x i64> [[VU64]], <vscale x 1 x i64> [[VU64]])
// CHECK-NEXT:    [[ELT_MAX8:%.*]] = call <vscale x 4 x half> @llvm.maxnum.nxv4f16(<vscale x 4 x half> [[VF16]], <vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_MAX9:%.*]] = call <vscale x 2 x float> @llvm.maxnum.nxv2f32(<vscale x 2 x float> [[VF32]], <vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_MAX10:%.*]] = call <vscale x 1 x double> @llvm.maxnum.nxv1f64(<vscale x 1 x double> [[VF64]], <vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_max(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vint8m1_t res_vi8 = __builtin_elementwise_max(vi8, vi8);
  vint16m1_t res_vi16 = __builtin_elementwise_max(vi16, vi16);
  vint32m1_t res_vi32 = __builtin_elementwise_max(vi32, vi32);
  vint64m1_t res_vi64 = __builtin_elementwise_max(vi64, vi64);
  vuint8m1_t res_vu8 = __builtin_elementwise_max(vu8, vu8);
  vuint16m1_t res_vu16 = __builtin_elementwise_max(vu16, vu16);
  vuint32m1_t res_vu32 = __builtin_elementwise_max(vu32, vu32);
  vuint64m1_t res_vu64 = __builtin_elementwise_max(vu64, vu64);
  vfloat16m1_t res_vf16 = __builtin_elementwise_max(vf16, vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_max(vf32, vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_max(vf64, vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_min
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_MIN:%.*]] = call <vscale x 8 x i8> @llvm.umin.nxv8i8(<vscale x 8 x i8> [[VI8]], <vscale x 8 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_MIN1:%.*]] = call <vscale x 4 x i16> @llvm.umin.nxv4i16(<vscale x 4 x i16> [[VI16]], <vscale x 4 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_MIN2:%.*]] = call <vscale x 2 x i32> @llvm.umin.nxv2i32(<vscale x 2 x i32> [[VI32]], <vscale x 2 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_MIN3:%.*]] = call <vscale x 1 x i64> @llvm.umin.nxv1i64(<vscale x 1 x i64> [[VI64]], <vscale x 1 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_MIN4:%.*]] = call <vscale x 8 x i8> @llvm.umin.nxv8i8(<vscale x 8 x i8> [[VU8]], <vscale x 8 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_MIN5:%.*]] = call <vscale x 4 x i16> @llvm.umin.nxv4i16(<vscale x 4 x i16> [[VU16]], <vscale x 4 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_MIN6:%.*]] = call <vscale x 2 x i32> @llvm.umin.nxv2i32(<vscale x 2 x i32> [[VU32]], <vscale x 2 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_MIN7:%.*]] = call <vscale x 1 x i64> @llvm.umin.nxv1i64(<vscale x 1 x i64> [[VU64]], <vscale x 1 x i64> [[VU64]])
// CHECK-NEXT:    [[ELT_MIN8:%.*]] = call <vscale x 4 x half> @llvm.minnum.nxv4f16(<vscale x 4 x half> [[VF16]], <vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_MIN9:%.*]] = call <vscale x 2 x float> @llvm.minnum.nxv2f32(<vscale x 2 x float> [[VF32]], <vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_MIN10:%.*]] = call <vscale x 1 x double> @llvm.minnum.nxv1f64(<vscale x 1 x double> [[VF64]], <vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_min(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vint8m1_t res_vi8 = __builtin_elementwise_min(vi8, vi8);
  vint16m1_t res_vi16 = __builtin_elementwise_min(vi16, vi16);
  vint32m1_t res_vi32 = __builtin_elementwise_min(vi32, vi32);
  vint64m1_t res_vi64 = __builtin_elementwise_min(vi64, vi64);
  vuint8m1_t res_vu8 = __builtin_elementwise_min(vu8, vu8);
  vuint16m1_t res_vu16 = __builtin_elementwise_min(vu16, vu16);
  vuint32m1_t res_vu32 = __builtin_elementwise_min(vu32, vu32);
  vuint64m1_t res_vu64 = __builtin_elementwise_min(vu64, vu64);
  vfloat16m1_t res_vf16 = __builtin_elementwise_min(vf16, vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_min(vf32, vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_min(vf64, vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_bitreverse
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_BITREVERSE:%.*]] = call <vscale x 8 x i8> @llvm.bitreverse.nxv8i8(<vscale x 8 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_BITREVERSE1:%.*]] = call <vscale x 4 x i16> @llvm.bitreverse.nxv4i16(<vscale x 4 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_BITREVERSE2:%.*]] = call <vscale x 2 x i32> @llvm.bitreverse.nxv2i32(<vscale x 2 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_BITREVERSE3:%.*]] = call <vscale x 1 x i64> @llvm.bitreverse.nxv1i64(<vscale x 1 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_BITREVERSE4:%.*]] = call <vscale x 8 x i8> @llvm.bitreverse.nxv8i8(<vscale x 8 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_BITREVERSE5:%.*]] = call <vscale x 4 x i16> @llvm.bitreverse.nxv4i16(<vscale x 4 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_BITREVERSE6:%.*]] = call <vscale x 2 x i32> @llvm.bitreverse.nxv2i32(<vscale x 2 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_BITREVERSE7:%.*]] = call <vscale x 1 x i64> @llvm.bitreverse.nxv1i64(<vscale x 1 x i64> [[VU64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_bitreverse(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vint8m1_t res_vi8 = __builtin_elementwise_bitreverse(vi8);
  vint16m1_t res_vi16 = __builtin_elementwise_bitreverse(vi16);
  vint32m1_t res_vi32 = __builtin_elementwise_bitreverse(vi32);
  vint64m1_t res_vi64 = __builtin_elementwise_bitreverse(vi64);
  vuint8m1_t res_vu8 = __builtin_elementwise_bitreverse(vu8);
  vuint16m1_t res_vu16 = __builtin_elementwise_bitreverse(vu16);
  vuint32m1_t res_vu32 = __builtin_elementwise_bitreverse(vu32);
  vuint64m1_t res_vu64 = __builtin_elementwise_bitreverse(vu64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_ceil
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_CEIL:%.*]] = call <vscale x 4 x half> @llvm.ceil.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_CEIL1:%.*]] = call <vscale x 2 x float> @llvm.ceil.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_CEIL2:%.*]] = call <vscale x 1 x double> @llvm.ceil.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_ceil(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_ceil(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_ceil(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_ceil(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_cos
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_COS:%.*]] = call <vscale x 4 x half> @llvm.cos.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_COS1:%.*]] = call <vscale x 2 x float> @llvm.cos.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_COS2:%.*]] = call <vscale x 1 x double> @llvm.cos.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_cos(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_cos(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_cos(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_cos(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_exp
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_EXP:%.*]] = call <vscale x 4 x half> @llvm.exp.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_EXP1:%.*]] = call <vscale x 2 x float> @llvm.exp.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_EXP2:%.*]] = call <vscale x 1 x double> @llvm.exp.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_exp(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_exp(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_exp(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_exp(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_exp2
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_EXP2:%.*]] = call <vscale x 4 x half> @llvm.exp2.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_EXP21:%.*]] = call <vscale x 2 x float> @llvm.exp2.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_EXP22:%.*]] = call <vscale x 1 x double> @llvm.exp2.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_exp2(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_exp2(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_exp2(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_exp2(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_floor
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_FLOOR:%.*]] = call <vscale x 4 x half> @llvm.floor.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_FLOOR1:%.*]] = call <vscale x 2 x float> @llvm.floor.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_FLOOR2:%.*]] = call <vscale x 1 x double> @llvm.floor.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_floor(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_floor(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_floor(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_floor(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_log
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_LOG:%.*]] = call <vscale x 4 x half> @llvm.log.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_LOG1:%.*]] = call <vscale x 2 x float> @llvm.log.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_LOG2:%.*]] = call <vscale x 1 x double> @llvm.log.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_log(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_log(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_log(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_log(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_log10
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_LOG10:%.*]] = call <vscale x 4 x half> @llvm.log10.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_LOG101:%.*]] = call <vscale x 2 x float> @llvm.log10.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_LOG102:%.*]] = call <vscale x 1 x double> @llvm.log10.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_log10(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_log10(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_log10(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_log10(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_log2
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_LOG2:%.*]] = call <vscale x 4 x half> @llvm.log2.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_LOG21:%.*]] = call <vscale x 2 x float> @llvm.log2.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_LOG22:%.*]] = call <vscale x 1 x double> @llvm.log2.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_log2(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_log2(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_log2(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_log2(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_pow
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 4 x half> @llvm.pow.nxv4f16(<vscale x 4 x half> [[VF16]], <vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x float> @llvm.pow.nxv2f32(<vscale x 2 x float> [[VF32]], <vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 1 x double> @llvm.pow.nxv1f64(<vscale x 1 x double> [[VF64]], <vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_pow(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_pow(vf16, vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_pow(vf32, vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_pow(vf64, vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_roundeven
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_ROUNDEVEN:%.*]] = call <vscale x 4 x half> @llvm.roundeven.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_ROUNDEVEN1:%.*]] = call <vscale x 2 x float> @llvm.roundeven.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_ROUNDEVEN2:%.*]] = call <vscale x 1 x double> @llvm.roundeven.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_roundeven(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_roundeven(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_roundeven(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_roundeven(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_round
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_ROUND:%.*]] = call <vscale x 4 x half> @llvm.round.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_ROUND1:%.*]] = call <vscale x 2 x float> @llvm.round.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_ROUND2:%.*]] = call <vscale x 1 x double> @llvm.round.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_round(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_round(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_round(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_round(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_rint
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_RINT:%.*]] = call <vscale x 4 x half> @llvm.rint.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_RINT1:%.*]] = call <vscale x 2 x float> @llvm.rint.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_RINT2:%.*]] = call <vscale x 1 x double> @llvm.rint.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_rint(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_rint(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_rint(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_rint(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_nearbyint
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_NEARBYINT:%.*]] = call <vscale x 4 x half> @llvm.nearbyint.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_NEARBYINT1:%.*]] = call <vscale x 2 x float> @llvm.nearbyint.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_NEARBYINT2:%.*]] = call <vscale x 1 x double> @llvm.nearbyint.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_nearbyint(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_nearbyint(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_nearbyint(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_nearbyint(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_sin
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_SIN:%.*]] = call <vscale x 4 x half> @llvm.sin.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_SIN1:%.*]] = call <vscale x 2 x float> @llvm.sin.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_SIN2:%.*]] = call <vscale x 1 x double> @llvm.sin.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_sin(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_sin(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_sin(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_sin(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_sqrt
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 4 x half> @llvm.sqrt.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x float> @llvm.sqrt.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 1 x double> @llvm.sqrt.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_sqrt(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_sqrt(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_sqrt(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_sqrt(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_trunc
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_TRUNC:%.*]] = call <vscale x 4 x half> @llvm.trunc.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_TRUNC1:%.*]] = call <vscale x 2 x float> @llvm.trunc.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_TRUNC2:%.*]] = call <vscale x 1 x double> @llvm.trunc.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_trunc(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_trunc(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_trunc(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_trunc(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_canonicalize
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_CANONICALIZE:%.*]] = call <vscale x 4 x half> @llvm.canonicalize.nxv4f16(<vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_CANONICALIZE1:%.*]] = call <vscale x 2 x float> @llvm.canonicalize.nxv2f32(<vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_CANONICALIZE2:%.*]] = call <vscale x 1 x double> @llvm.canonicalize.nxv1f64(<vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_canonicalize(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_canonicalize(vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_canonicalize(vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_canonicalize(vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_copysign
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 4 x half> @llvm.copysign.nxv4f16(<vscale x 4 x half> [[VF16]], <vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x float> @llvm.copysign.nxv2f32(<vscale x 2 x float> [[VF32]], <vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 1 x double> @llvm.copysign.nxv1f64(<vscale x 1 x double> [[VF64]], <vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_copysign(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_copysign(vf16, vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_copysign(vf32, vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_copysign(vf64, vf64);
}

// CHECK-LABEL: define dso_local void @test_builtin_elementwise_fma
// CHECK-SAME: (<vscale x 8 x i8> [[VI8:%.*]], <vscale x 4 x i16> [[VI16:%.*]], <vscale x 2 x i32> [[VI32:%.*]], <vscale x 1 x i64> [[VI64:%.*]], <vscale x 8 x i8> [[VU8:%.*]], <vscale x 4 x i16> [[VU16:%.*]], <vscale x 2 x i32> [[VU32:%.*]], <vscale x 1 x i64> [[VU64:%.*]], <vscale x 4 x half> [[VF16:%.*]], <vscale x 2 x float> [[VF32:%.*]], <vscale x 1 x double> [[VF64:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 4 x half> @llvm.fma.nxv4f16(<vscale x 4 x half> [[VF16]], <vscale x 4 x half> [[VF16]], <vscale x 4 x half> [[VF16]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x float> @llvm.fma.nxv2f32(<vscale x 2 x float> [[VF32]], <vscale x 2 x float> [[VF32]], <vscale x 2 x float> [[VF32]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 1 x double> @llvm.fma.nxv1f64(<vscale x 1 x double> [[VF64]], <vscale x 1 x double> [[VF64]], <vscale x 1 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_fma(vint8m1_t vi8, vint16m1_t vi16,
                                  vint32m1_t vi32, vint64m1_t vi64,
                                  vuint8m1_t vu8, vuint16m1_t vu16,
                                  vuint32m1_t vu32, vuint64m1_t vu64,
                                  vfloat16m1_t vf16, vfloat32m1_t vf32,
                                  vfloat64m1_t vf64) {
  vfloat16m1_t res_vf16 = __builtin_elementwise_fma(vf16, vf16, vf16);
  vfloat32m1_t res_vf32 = __builtin_elementwise_fma(vf32, vf32, vf32);
  vfloat64m1_t res_vf64 = __builtin_elementwise_fma(vf64, vf64, vf64);
}
