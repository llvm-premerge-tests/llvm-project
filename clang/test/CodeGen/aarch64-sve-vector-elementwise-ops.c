// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple aarch64-none-linux-gnu -target-feature +sve \
// RUN: -disable-O0-optnone \
// RUN:  -emit-llvm -o - %s | opt -S -passes=sroa | FileCheck %s

// REQUIRES: aarch64-registered-target

#include <arm_sve.h>

// CHECK-LABEL: @test_builtin_elementwise_abs(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_ABS:%.*]] = call <vscale x 16 x i8> @llvm.abs.nxv16i8(<vscale x 16 x i8> [[VI8:%.*]], i1 false)
// CHECK-NEXT:    [[ELT_ABS1:%.*]] = call <vscale x 8 x i16> @llvm.abs.nxv8i16(<vscale x 8 x i16> [[VI16:%.*]], i1 false)
// CHECK-NEXT:    [[ELT_ABS2:%.*]] = call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[VI32:%.*]], i1 false)
// CHECK-NEXT:    [[ELT_ABS3:%.*]] = call <vscale x 2 x i64> @llvm.abs.nxv2i64(<vscale x 2 x i64> [[VI64:%.*]], i1 false)
// CHECK-NEXT:    [[ELT_ABS4:%.*]] = call <vscale x 8 x half> @llvm.fabs.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_ABS5:%.*]] = call <vscale x 4 x float> @llvm.fabs.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_ABS6:%.*]] = call <vscale x 2 x double> @llvm.fabs.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_abs(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svint8_t res_vi8 = __builtin_elementwise_abs(vi8);
  svint16_t res_vi16 = __builtin_elementwise_abs(vi16);
  svint32_t res_vi32 = __builtin_elementwise_abs(vi32);
  svint64_t res_vi64 = __builtin_elementwise_abs(vi64);
  svfloat16_t res_vf16 = __builtin_elementwise_abs(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_abs(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_abs(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_add_sat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_SAT:%.*]] = call <vscale x 16 x i8> @llvm.sadd.sat.nxv16i8(<vscale x 16 x i8> [[VI8:%.*]], <vscale x 16 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_SAT1:%.*]] = call <vscale x 8 x i16> @llvm.sadd.sat.nxv8i16(<vscale x 8 x i16> [[VI16:%.*]], <vscale x 8 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_SAT2:%.*]] = call <vscale x 4 x i32> @llvm.sadd.sat.nxv4i32(<vscale x 4 x i32> [[VI32:%.*]], <vscale x 4 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_SAT3:%.*]] = call <vscale x 2 x i64> @llvm.sadd.sat.nxv2i64(<vscale x 2 x i64> [[VI64:%.*]], <vscale x 2 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_SAT4:%.*]] = call <vscale x 16 x i8> @llvm.uadd.sat.nxv16i8(<vscale x 16 x i8> [[VU8:%.*]], <vscale x 16 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_SAT5:%.*]] = call <vscale x 8 x i16> @llvm.uadd.sat.nxv8i16(<vscale x 8 x i16> [[VU16:%.*]], <vscale x 8 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_SAT6:%.*]] = call <vscale x 4 x i32> @llvm.uadd.sat.nxv4i32(<vscale x 4 x i32> [[VU32:%.*]], <vscale x 4 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_SAT7:%.*]] = call <vscale x 2 x i64> @llvm.uadd.sat.nxv2i64(<vscale x 2 x i64> [[VU64:%.*]], <vscale x 2 x i64> [[VU64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_add_sat(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svint8_t res_vi8 = __builtin_elementwise_add_sat(vi8, vi8);
  svint16_t res_vi16 = __builtin_elementwise_add_sat(vi16, vi16);
  svint32_t res_vi32 = __builtin_elementwise_add_sat(vi32, vi32);
  svint64_t res_vi64 = __builtin_elementwise_add_sat(vi64, vi64);
  svuint8_t res_vu8 = __builtin_elementwise_add_sat(vu8, vu8);
  svuint16_t res_vu16 = __builtin_elementwise_add_sat(vu16, vu16);
  svuint32_t res_vu32 = __builtin_elementwise_add_sat(vu32, vu32);
  svuint64_t res_vu64 = __builtin_elementwise_add_sat(vu64, vu64);
}

// CHECK-LABEL: @test_builtin_elementwise_sub_sat(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_SAT:%.*]] = call <vscale x 16 x i8> @llvm.ssub.sat.nxv16i8(<vscale x 16 x i8> [[VI8:%.*]], <vscale x 16 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_SAT1:%.*]] = call <vscale x 8 x i16> @llvm.ssub.sat.nxv8i16(<vscale x 8 x i16> [[VI16:%.*]], <vscale x 8 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_SAT2:%.*]] = call <vscale x 4 x i32> @llvm.ssub.sat.nxv4i32(<vscale x 4 x i32> [[VI32:%.*]], <vscale x 4 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_SAT3:%.*]] = call <vscale x 2 x i64> @llvm.ssub.sat.nxv2i64(<vscale x 2 x i64> [[VI64:%.*]], <vscale x 2 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_SAT4:%.*]] = call <vscale x 16 x i8> @llvm.usub.sat.nxv16i8(<vscale x 16 x i8> [[VU8:%.*]], <vscale x 16 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_SAT5:%.*]] = call <vscale x 8 x i16> @llvm.usub.sat.nxv8i16(<vscale x 8 x i16> [[VU16:%.*]], <vscale x 8 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_SAT6:%.*]] = call <vscale x 4 x i32> @llvm.usub.sat.nxv4i32(<vscale x 4 x i32> [[VU32:%.*]], <vscale x 4 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_SAT7:%.*]] = call <vscale x 2 x i64> @llvm.usub.sat.nxv2i64(<vscale x 2 x i64> [[VU64:%.*]], <vscale x 2 x i64> [[VU64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_sub_sat(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svint8_t res_vi8 = __builtin_elementwise_sub_sat(vi8, vi8);
  svint16_t res_vi16 = __builtin_elementwise_sub_sat(vi16, vi16);
  svint32_t res_vi32 = __builtin_elementwise_sub_sat(vi32, vi32);
  svint64_t res_vi64 = __builtin_elementwise_sub_sat(vi64, vi64);
  svuint8_t res_vu8 = __builtin_elementwise_sub_sat(vu8, vu8);
  svuint16_t res_vu16 = __builtin_elementwise_sub_sat(vu16, vu16);
  svuint32_t res_vu32 = __builtin_elementwise_sub_sat(vu32, vu32);
  svuint64_t res_vu64 = __builtin_elementwise_sub_sat(vu64, vu64);
}

// CHECK-LABEL: @test_builtin_elementwise_max(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_MAX:%.*]] = call <vscale x 16 x i8> @llvm.smax.nxv16i8(<vscale x 16 x i8> [[VI8:%.*]], <vscale x 16 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_MAX1:%.*]] = call <vscale x 8 x i16> @llvm.smax.nxv8i16(<vscale x 8 x i16> [[VI16:%.*]], <vscale x 8 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_MAX2:%.*]] = call <vscale x 4 x i32> @llvm.smax.nxv4i32(<vscale x 4 x i32> [[VI32:%.*]], <vscale x 4 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_MAX3:%.*]] = call <vscale x 2 x i64> @llvm.smax.nxv2i64(<vscale x 2 x i64> [[VI64:%.*]], <vscale x 2 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_MAX4:%.*]] = call <vscale x 16 x i8> @llvm.umax.nxv16i8(<vscale x 16 x i8> [[VU8:%.*]], <vscale x 16 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_MAX5:%.*]] = call <vscale x 8 x i16> @llvm.umax.nxv8i16(<vscale x 8 x i16> [[VU16:%.*]], <vscale x 8 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_MAX6:%.*]] = call <vscale x 4 x i32> @llvm.umax.nxv4i32(<vscale x 4 x i32> [[VU32:%.*]], <vscale x 4 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_MAX7:%.*]] = call <vscale x 2 x i64> @llvm.umax.nxv2i64(<vscale x 2 x i64> [[VU64:%.*]], <vscale x 2 x i64> [[VU64]])
// CHECK-NEXT:    [[ELT_MAX8:%.*]] = call <vscale x 8 x half> @llvm.maxnum.nxv8f16(<vscale x 8 x half> [[VF16:%.*]], <vscale x 8 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_MAX9:%.*]] = call <vscale x 4 x float> @llvm.maxnum.nxv4f32(<vscale x 4 x float> [[VF32:%.*]], <vscale x 4 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_MAX10:%.*]] = call <vscale x 2 x double> @llvm.maxnum.nxv2f64(<vscale x 2 x double> [[VF64:%.*]], <vscale x 2 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_max(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svint8_t res_vi8 = __builtin_elementwise_max(vi8, vi8);
  svint16_t res_vi16 = __builtin_elementwise_max(vi16, vi16);
  svint32_t res_vi32 = __builtin_elementwise_max(vi32, vi32);
  svint64_t res_vi64 = __builtin_elementwise_max(vi64, vi64);
  svuint8_t res_vu8 = __builtin_elementwise_max(vu8, vu8);
  svuint16_t res_vu16 = __builtin_elementwise_max(vu16, vu16);
  svuint32_t res_vu32 = __builtin_elementwise_max(vu32, vu32);
  svuint64_t res_vu64 = __builtin_elementwise_max(vu64, vu64);
  svfloat16_t res_vf16 = __builtin_elementwise_max(vf16, vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_max(vf32, vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_max(vf64, vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_min(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_MIN:%.*]] = call <vscale x 16 x i8> @llvm.umin.nxv16i8(<vscale x 16 x i8> [[VI8:%.*]], <vscale x 16 x i8> [[VI8]])
// CHECK-NEXT:    [[ELT_MIN1:%.*]] = call <vscale x 8 x i16> @llvm.umin.nxv8i16(<vscale x 8 x i16> [[VI16:%.*]], <vscale x 8 x i16> [[VI16]])
// CHECK-NEXT:    [[ELT_MIN2:%.*]] = call <vscale x 4 x i32> @llvm.umin.nxv4i32(<vscale x 4 x i32> [[VI32:%.*]], <vscale x 4 x i32> [[VI32]])
// CHECK-NEXT:    [[ELT_MIN3:%.*]] = call <vscale x 2 x i64> @llvm.umin.nxv2i64(<vscale x 2 x i64> [[VI64:%.*]], <vscale x 2 x i64> [[VI64]])
// CHECK-NEXT:    [[ELT_MIN4:%.*]] = call <vscale x 16 x i8> @llvm.umin.nxv16i8(<vscale x 16 x i8> [[VU8:%.*]], <vscale x 16 x i8> [[VU8]])
// CHECK-NEXT:    [[ELT_MIN5:%.*]] = call <vscale x 8 x i16> @llvm.umin.nxv8i16(<vscale x 8 x i16> [[VU16:%.*]], <vscale x 8 x i16> [[VU16]])
// CHECK-NEXT:    [[ELT_MIN6:%.*]] = call <vscale x 4 x i32> @llvm.umin.nxv4i32(<vscale x 4 x i32> [[VU32:%.*]], <vscale x 4 x i32> [[VU32]])
// CHECK-NEXT:    [[ELT_MIN7:%.*]] = call <vscale x 2 x i64> @llvm.umin.nxv2i64(<vscale x 2 x i64> [[VU64:%.*]], <vscale x 2 x i64> [[VU64]])
// CHECK-NEXT:    [[ELT_MIN8:%.*]] = call <vscale x 8 x half> @llvm.minnum.nxv8f16(<vscale x 8 x half> [[VF16:%.*]], <vscale x 8 x half> [[VF16]])
// CHECK-NEXT:    [[ELT_MIN9:%.*]] = call <vscale x 4 x float> @llvm.minnum.nxv4f32(<vscale x 4 x float> [[VF32:%.*]], <vscale x 4 x float> [[VF32]])
// CHECK-NEXT:    [[ELT_MIN10:%.*]] = call <vscale x 2 x double> @llvm.minnum.nxv2f64(<vscale x 2 x double> [[VF64:%.*]], <vscale x 2 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_min(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svint8_t res_vi8 = __builtin_elementwise_min(vi8, vi8);
  svint16_t res_vi16 = __builtin_elementwise_min(vi16, vi16);
  svint32_t res_vi32 = __builtin_elementwise_min(vi32, vi32);
  svint64_t res_vi64 = __builtin_elementwise_min(vi64, vi64);
  svuint8_t res_vu8 = __builtin_elementwise_min(vu8, vu8);
  svuint16_t res_vu16 = __builtin_elementwise_min(vu16, vu16);
  svuint32_t res_vu32 = __builtin_elementwise_min(vu32, vu32);
  svuint64_t res_vu64 = __builtin_elementwise_min(vu64, vu64);
  svfloat16_t res_vf16 = __builtin_elementwise_min(vf16, vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_min(vf32, vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_min(vf64, vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_bitreverse(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_BITREVERSE:%.*]] = call <vscale x 16 x i8> @llvm.bitreverse.nxv16i8(<vscale x 16 x i8> [[VI8:%.*]])
// CHECK-NEXT:    [[ELT_BITREVERSE1:%.*]] = call <vscale x 8 x i16> @llvm.bitreverse.nxv8i16(<vscale x 8 x i16> [[VI16:%.*]])
// CHECK-NEXT:    [[ELT_BITREVERSE2:%.*]] = call <vscale x 4 x i32> @llvm.bitreverse.nxv4i32(<vscale x 4 x i32> [[VI32:%.*]])
// CHECK-NEXT:    [[ELT_BITREVERSE3:%.*]] = call <vscale x 2 x i64> @llvm.bitreverse.nxv2i64(<vscale x 2 x i64> [[VI64:%.*]])
// CHECK-NEXT:    [[ELT_BITREVERSE4:%.*]] = call <vscale x 16 x i8> @llvm.bitreverse.nxv16i8(<vscale x 16 x i8> [[VU8:%.*]])
// CHECK-NEXT:    [[ELT_BITREVERSE5:%.*]] = call <vscale x 8 x i16> @llvm.bitreverse.nxv8i16(<vscale x 8 x i16> [[VU16:%.*]])
// CHECK-NEXT:    [[ELT_BITREVERSE6:%.*]] = call <vscale x 4 x i32> @llvm.bitreverse.nxv4i32(<vscale x 4 x i32> [[VU32:%.*]])
// CHECK-NEXT:    [[ELT_BITREVERSE7:%.*]] = call <vscale x 2 x i64> @llvm.bitreverse.nxv2i64(<vscale x 2 x i64> [[VU64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_bitreverse(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svint8_t res_vi8 = __builtin_elementwise_bitreverse(vi8);
  svint16_t res_vi16 = __builtin_elementwise_bitreverse(vi16);
  svint32_t res_vi32 = __builtin_elementwise_bitreverse(vi32);
  svint64_t res_vi64 = __builtin_elementwise_bitreverse(vi64);
  svuint8_t res_vu8 = __builtin_elementwise_bitreverse(vu8);
  svuint16_t res_vu16 = __builtin_elementwise_bitreverse(vu16);
  svuint32_t res_vu32 = __builtin_elementwise_bitreverse(vu32);
  svuint64_t res_vu64 = __builtin_elementwise_bitreverse(vu64);
}

// CHECK-LABEL: @test_builtin_elementwise_ceil(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_CEIL:%.*]] = call <vscale x 8 x half> @llvm.ceil.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_CEIL1:%.*]] = call <vscale x 4 x float> @llvm.ceil.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_CEIL2:%.*]] = call <vscale x 2 x double> @llvm.ceil.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_ceil(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_ceil(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_ceil(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_ceil(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_cos(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_COS:%.*]] = call <vscale x 8 x half> @llvm.cos.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_COS1:%.*]] = call <vscale x 4 x float> @llvm.cos.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_COS2:%.*]] = call <vscale x 2 x double> @llvm.cos.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_cos(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_cos(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_cos(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_cos(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_exp(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_EXP:%.*]] = call <vscale x 8 x half> @llvm.exp.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_EXP1:%.*]] = call <vscale x 4 x float> @llvm.exp.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_EXP2:%.*]] = call <vscale x 2 x double> @llvm.exp.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_exp(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_exp(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_exp(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_exp(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_exp2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_EXP2:%.*]] = call <vscale x 8 x half> @llvm.exp2.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_EXP21:%.*]] = call <vscale x 4 x float> @llvm.exp2.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_EXP22:%.*]] = call <vscale x 2 x double> @llvm.exp2.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_exp2(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_exp2(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_exp2(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_exp2(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_floor(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_FLOOR:%.*]] = call <vscale x 8 x half> @llvm.floor.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_FLOOR1:%.*]] = call <vscale x 4 x float> @llvm.floor.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_FLOOR2:%.*]] = call <vscale x 2 x double> @llvm.floor.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_floor(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_floor(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_floor(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_floor(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_log(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_LOG:%.*]] = call <vscale x 8 x half> @llvm.log.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_LOG1:%.*]] = call <vscale x 4 x float> @llvm.log.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_LOG2:%.*]] = call <vscale x 2 x double> @llvm.log.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_log(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_log(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_log(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_log(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_log10(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_LOG10:%.*]] = call <vscale x 8 x half> @llvm.log10.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_LOG101:%.*]] = call <vscale x 4 x float> @llvm.log10.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_LOG102:%.*]] = call <vscale x 2 x double> @llvm.log10.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_log10(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_log10(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_log10(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_log10(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_log2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_LOG2:%.*]] = call <vscale x 8 x half> @llvm.log2.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_LOG21:%.*]] = call <vscale x 4 x float> @llvm.log2.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_LOG22:%.*]] = call <vscale x 2 x double> @llvm.log2.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_log2(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_log2(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_log2(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_log2(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_pow(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 8 x half> @llvm.pow.nxv8f16(<vscale x 8 x half> [[VF16:%.*]], <vscale x 8 x half> [[VF16]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x float> @llvm.pow.nxv4f32(<vscale x 4 x float> [[VF32:%.*]], <vscale x 4 x float> [[VF32]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x double> @llvm.pow.nxv2f64(<vscale x 2 x double> [[VF64:%.*]], <vscale x 2 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_pow(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_pow(vf16, vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_pow(vf32, vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_pow(vf64, vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_roundeven(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_ROUNDEVEN:%.*]] = call <vscale x 8 x half> @llvm.roundeven.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_ROUNDEVEN1:%.*]] = call <vscale x 4 x float> @llvm.roundeven.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_ROUNDEVEN2:%.*]] = call <vscale x 2 x double> @llvm.roundeven.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_roundeven(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_roundeven(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_roundeven(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_roundeven(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_round(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_ROUND:%.*]] = call <vscale x 8 x half> @llvm.round.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_ROUND1:%.*]] = call <vscale x 4 x float> @llvm.round.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_ROUND2:%.*]] = call <vscale x 2 x double> @llvm.round.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_round(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_round(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_round(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_round(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_rint(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_RINT:%.*]] = call <vscale x 8 x half> @llvm.rint.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_RINT1:%.*]] = call <vscale x 4 x float> @llvm.rint.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_RINT2:%.*]] = call <vscale x 2 x double> @llvm.rint.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_rint(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_rint(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_rint(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_rint(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_nearbyint(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_NEARBYINT:%.*]] = call <vscale x 8 x half> @llvm.nearbyint.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_NEARBYINT1:%.*]] = call <vscale x 4 x float> @llvm.nearbyint.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_NEARBYINT2:%.*]] = call <vscale x 2 x double> @llvm.nearbyint.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_nearbyint(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_nearbyint(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_nearbyint(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_nearbyint(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_sin(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_SIN:%.*]] = call <vscale x 8 x half> @llvm.sin.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_SIN1:%.*]] = call <vscale x 4 x float> @llvm.sin.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_SIN2:%.*]] = call <vscale x 2 x double> @llvm.sin.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_sin(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_sin(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_sin(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_sin(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_sqrt(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 8 x half> @llvm.sqrt.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x float> @llvm.sqrt.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x double> @llvm.sqrt.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_sqrt(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_sqrt(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_sqrt(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_sqrt(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_trunc(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_TRUNC:%.*]] = call <vscale x 8 x half> @llvm.trunc.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_TRUNC1:%.*]] = call <vscale x 4 x float> @llvm.trunc.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_TRUNC2:%.*]] = call <vscale x 2 x double> @llvm.trunc.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_trunc(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_trunc(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_trunc(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_trunc(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_canonicalize(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ELT_CANONICALIZE:%.*]] = call <vscale x 8 x half> @llvm.canonicalize.nxv8f16(<vscale x 8 x half> [[VF16:%.*]])
// CHECK-NEXT:    [[ELT_CANONICALIZE1:%.*]] = call <vscale x 4 x float> @llvm.canonicalize.nxv4f32(<vscale x 4 x float> [[VF32:%.*]])
// CHECK-NEXT:    [[ELT_CANONICALIZE2:%.*]] = call <vscale x 2 x double> @llvm.canonicalize.nxv2f64(<vscale x 2 x double> [[VF64:%.*]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_canonicalize(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_canonicalize(vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_canonicalize(vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_canonicalize(vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_copysign(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 8 x half> @llvm.copysign.nxv8f16(<vscale x 8 x half> [[VF16:%.*]], <vscale x 8 x half> [[VF16]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x float> @llvm.copysign.nxv4f32(<vscale x 4 x float> [[VF32:%.*]], <vscale x 4 x float> [[VF32]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x double> @llvm.copysign.nxv2f64(<vscale x 2 x double> [[VF64:%.*]], <vscale x 2 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_copysign(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_copysign(vf16, vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_copysign(vf32, vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_copysign(vf64, vf64);
}

// CHECK-LABEL: @test_builtin_elementwise_fma(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 8 x half> @llvm.fma.nxv8f16(<vscale x 8 x half> [[VF16:%.*]], <vscale x 8 x half> [[VF16]], <vscale x 8 x half> [[VF16]])
// CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x float> @llvm.fma.nxv4f32(<vscale x 4 x float> [[VF32:%.*]], <vscale x 4 x float> [[VF32]], <vscale x 4 x float> [[VF32]])
// CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x double> @llvm.fma.nxv2f64(<vscale x 2 x double> [[VF64:%.*]], <vscale x 2 x double> [[VF64]], <vscale x 2 x double> [[VF64]])
// CHECK-NEXT:    ret void
//
void test_builtin_elementwise_fma(svint8_t vi8, svint16_t vi16,
                                  svint32_t vi32, svint64_t vi64,
                                  svuint8_t vu8, svuint16_t vu16,
                                  svuint32_t vu32, svuint64_t vu64,
                                  svfloat16_t vf16, svfloat32_t vf32,
                                  svfloat64_t vf64) {
  svfloat16_t res_vf16 = __builtin_elementwise_fma(vf16, vf16, vf16);
  svfloat32_t res_vf32 = __builtin_elementwise_fma(vf32, vf32, vf32);
  svfloat64_t res_vf64 = __builtin_elementwise_fma(vf64, vf64, vf64);
}
